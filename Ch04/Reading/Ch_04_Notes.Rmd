---
title: "Chapter 4 Notes"
author: "Darrell A. Carvalho"
output:
  html_document: 
    toc: yes
    toc_depth: 4
    toc_float: yes
    theme: spacelab
    number_sections: yes
---

# Setup

```{r setup, eval = TRUE, include = TRUE}
knitr::opts_chunk$set(echo = TRUE)
# install.packages("remotes")
# remotes::install_github("geocompx/geocompkg")
library(sf)
library(terra)
library(spData)
library(dplyr)
elev = rast(system.file("raster/elev.tif", package = "spData"))
grain = rast(system.file("raster/grain.tif", package = "spData"))
```

# Introduction

-   Chapter demonstrates **spatial operations** or "how spatial objects can be modified ... based on their location and shape."
    -   Included are **spatial subsetting**, **spatial joining**, and **spatial aggregation**
-   **Spatial joins** include a variety of means for joining - from **intersect** to **within distance** operations, as all spatial objects have a **spatial relationship**.
    -   distance calculations help us determine the strength and/or relevance of this relationship.
-   **Spatial operations** on raster objects:
    -   **subsetting**
    -   **merging**
    -   **Map Algebra**
        -   **local**
        -   **focal**
        -   **zonal**
        -   **global**

# Spatial operations on vector data

## Spatial Subsetting

-   **Spatial subsetting**: "process of taking a spatial object and returning ... only features that *relate* in space to another object."
    -   can be created with square brackets `[`
        -   syntax: `x[y, , op = st_intersects]`
            -   **x** is an `sf` object to be subset
            -   **y** is the 'subsetting object'
            -   `op = st_intersects` is an optional topological argument
    -   Demonstration of spatial subsetting using `nz` and `nz_height`

```{r canterbury-object}
canterbury <- nz |> filter(Name == "Canterbury")
canterbury_height <- nz_height[canterbury, ]
```

-   **spatial subsetting** can also be performed with **topological operators**
    -   the code chunk below returns a **sparse geometry binary predicate**
        -   `sgbp` are *sparse matrices*, lists of integer vectors with ordered, TRUE indices for each row.
        -   in other words, it does not store the entire matrix, instead storing only non-zero values and their location within a matrix, thus removing zero-values and their storage requirements.

```{r topological-operator}
sel_sgbp <- st_intersects(nz_height, canterbury)
sel_sgbp
```

```         
- This chunk can then be used to subset
```

```{r topological-operator-subsetting}
sel_logical <- lengths(sel_sgbp) > 0
canterbury_height2 <- nz_height[sel_logical, ]
```

```{r topological-operator-subetting-with-st_filter}
canterbury_height3 <- nz_height |>
  st_filter(y = canterbury, .predicate = st_intersects)
```

## Topological relations

-   A topological relationship defines spatial relations between objects.
-   **Binary topological relationships** are logical relations (`T` or `F`)
-   these logic statements are made about two objects
    -   the objects are defined by ordered sets of points that define the points, lines, and polygons.
-   `sf` has 'binary predicates' - functions that test topological relations

```{r topological-relations-demo}
# Create a polygon
polygon_matrix <- cbind(
  x = c(0,0,1,1,0),
  y = c(0,1,1,0.5,0)
)
polygon_sfc <- st_sfc(st_polygon(list(polygon_matrix)))

# create line
line_sfc <- st_sfc(st_linestring(cbind(
  x = c(0.4, 1),
  y = c(0.2, 0.5)
)))

# create points
point_df <- data.frame(
  x = c(0.2, 0.7, 0.4),
  y = c(0.1, 0.2, 0.8)
)

point_sf <- st_as_sf(point_df, coords = c("x", "y"))
```

```{r predicate-checks}
st_intersects(point_sf, polygon_sfc) # do any of the points intersect the poly?
st_within(point_sf, polygon_sfc) # are any of them within the polygon?
st_touches(point_sf, polygon_sfc) # do any points touch the polygon?
st_disjoint(point_sf, polygon_sfc) # are any of the points exist outside poly?
st_is_within_distance(point_sf, polygon_sfc, dist = 0.2) # are points w/i 0.2 unit distance of poly?
```

## DE-9IM Strings

## Spatial Joining

-   Spatial Joins use spatial predicates as keys, unlike non-spatial joins

```{r reprex}
set.seed(2018) # set seed for reproducibility
data(world)
(bb = st_bbox(world)) # the world's bounds
#>   xmin   ymin   xmax   ymax 
#> -180.0  -89.9  180.0   83.6
random_df = data.frame(
  x = runif(n = 10, min = bb[1], max = bb[3]),
  y = runif(n = 10, min = bb[2], max = bb[4])
)
random_points = random_df |> 
  st_as_sf(coords = c("x", "y"), crs = "EPSG:4326") # set coordinates and CRS

world_random = world[random_points, ]
nrow(world_random)
#> [1] 4
random_joined = st_join(random_points, world["name_long"])
```

```{r}
tmap::qtm(random_points)
tmap::qtm(world_random)
tmap::qtm(random_joined)
```

### Distance-based joins

-   Some datasets don't intersect, but still have a strong geographic relationship due to proximity

```{r cycle-prox}
plot(st_geometry(cycle_hire), col = "blue")
plot(st_geometry(cycle_hire_osm), add = TRUE, pch = 3, col = "red")
```

-   as we can see, the points show strong spatial relationships, but don't necessarily intersect
-   Consider a need to join `capacity` from `cycle_hire_osm` to `cycle_hire` - this needs a nonoverlapping join
    -   simplest method is use of `st_is_within_distance()`
        -   when the S2 spherical geometry engine is enabled, units can be set even when operating on unprojected data

```{r}
sel = st_is_within_distance(cycle_hire,
                            cycle_hire_osm,
                            dist = units::set_units(20, "m"))
summary(lengths(sel) > 0)
```

-   To retrieve values, we again use `st_join()` with a `dist` argument:

```{r}
z <- st_join(cycle_hire,
             cycle_hire_osm,
             st_is_within_distance,
             dist = units::set_units(
               20,"m"))
nrow(cycle_hire)
nrow(z)
```

-   Note how join has more rows than target. Some stations have multiple matches.
    -   We can aggregate using, e.g., mean

```{r}
z <- z %>% 
  group_by(id) %>% 
  summarize(capacity = mean(capacity))
nrow(z) == nrow(cycle_hire)
```

-   now we plot

```{r}
plot(cycle_hire_osm["capacity"])
plot(z["capacity"])
```

### Spatial Aggregation

-   Spatial data aggregation condenses data along a spatial variable, rather than attribute grouping
    -   e.g. average height of high points by region
        -   the geometry of the source determines grouping

```{r}
nz_agg <- aggregate(x = nz_height,
                    by = nz,
                    FUN = mean)

nz_agg %>% tmap::qtm('elevation')
```

-   we can create the same results by piping the `st_join` result to `group_by` and `summary`

```{r}
nz_agg2 <- st_join(x = nz,
                   y = nz_height) %>% 
  group_by(Name) %>% 
  summarize(elevation = mean(elevation, 
                             na.rm = TRUE))

nz_agg2 %>% tmap::qtm('elevation')
```

-   The difference between these two approaches:
    -   with `aggregate()`, NA values for unmatching region names
    -   with `group_by() %>% summarize()`, region names are preserved

### Joining incongruent layers

-   Object Y is congruent with object X if the two objects share borders.
    -   Incongruent objects do not share common borders
    -   When aggregating incongruent objects, we often rely on **areal interpolation**
-   We will load in datasets from **spData** named `incongruent` and `aggregating_zones`
    -   How do we transfer values from nine spatial polygons in `incongruent` to the two `aggregating zones`?
    -   We start with *area weighted spatial interpolation*
        -   transfers values in proportion with area of overlap

```{r}
iv <- incongruent["value"] # keep only values to be transferred
agg_aw <- st_interpolate_aw(iv, aggregating_zones, extensive = TRUE)
cat("\r\n") # inserts a new line for readability
agg_aw$value
```

-   This is meaningful as total income is "spatial extensive" - it increases as area increases
    -   with with "spatially intensive" variables (averages, percentages), the relationship between variable and area is not linear
        -   in this instance, we would need to use the `extensive = FALSE` parameter
            -   This will use an average rather than sum function when aggregating

## Spatial operations on raster data

```{r data-loading}
elev = rast(system.file("raster/elev.tif", package = "spData"))
grain = rast(system.file("raster/grain.tif", package = "spData"))
```

### Spatial subsetting

-   raster objects can be extracted by coordinates and other spatial objects
    -   We can use **terra** function `cellFromXY()` to turn coordinates into a cell ID
        -   alternatively, we can use `terra::extract()`

```{r}
## The following two codes give the equivalent result
id <- cellFromXY(elev, xy = matrix(c(0.1, 0.1), ncol = 2))
elev[id]

terra::extract(elev, matrix(c(0.1, 0.1), ncol = 2))
```

-   Raster objects can be subset by other raster objects:

```{r}
clip = rast(xmin = 0.9,
            xmax = 1.8,
            ymin = -0.45,
            ymax = 0.45,
            resolution = 0.3,
            vals = rep(1, 9))

elev[clip]

# we could also use terra::extract()

```

-   to get spatial outputs, we can set `drop` argument to `FALSE`

```{r}
elev[1:2, drop = FALSE]
```

-   We may want to use logical rasters to mask other rasters

```{r}
rmask = elev
vals <-  sample(c(FALSE, TRUE), 36, replace = TRUE)
rmask <- setValues(rmask, vals)

mask(elev, rmask, maskvalues = FALSE) %>% plot()

elev[rmask, drop = FALSE] %>% plot()


## We can also mask cells by value like so
# elev[elev < 20] = NA
# plot(elev)
```

### Map Algebra

-   term coined in 70's
    -   authors of book define 'map algebra' as "operations that modify or summarize raster cell values, with reference to surrounding cells, zones, or statistical functions that apply to every cell"
-   Map algebra has four sub-classes:
    -   *Local* - per-cell operations
    -   *Focal* - neighborhood operations; output is usually a result of 3x3 input cell block
    -   *Zonal* - similar to focal, but surrounding grid can be irregular
    -   *Global* - per-raster operations; output cell derived from one or several whole rasters

#### Local operations

-   cell-by-cell operations one one or more layers; includes raster addition or substraction, squaring, or multiplying
-   Also includes logical operations

```{r}
elev %>% plot()
(elev + elev) %>% plot() # raster addition
elev^2 %>% plot() # raster exponentiation
log(elev) %>% plot() # raster logging
(elev > 5) %>% plot() # raster logic operations
```

-   local operations can include interval reclassification/value grouping:

```{r}
# create a dem classification matrix - values between 0-12 are classed as 1, 12-24 are classed as 2, 24-36 as 3
rcl <- matrix(c(0, 12, 1, 12, 24, 2, 24, 36, 3), ncol = 3, byrow = TRUE)
rcl

recl <- classify(elev, rcl=rcl)
plot(recl)
```

-   one can also use `app()`, `tapp()`, and `lapp()` on rasters:
    -   `app()` apples function to each cell, and can sum multiple layers' values into one layer
    -   `tapp()` allows us to select a subset of layers to apply functions to
    -   `lapp()` allows us to apply a function to each cell treating layers as arguments
-   We will use `lapp()` to calculate NDVI

```{r}
# First we load the data
multi_raster_file <- system.file("raster/landsat.tif", package = "spDataLarge")
multi_rast <- rast(multi_raster_file)

# We need to convert the Landsat level-2 products from integer to float
# we will use a scaling factor = 0.0000275, and offset = -0.2
multi_rast = (multi_rast * 0.0000275) - 0.2
# remove invalid values
multi_rast[multi_rast < 0] = 0
```

```{r}
# Create an ndvi function
ndvi_fun = function(nir, red){
  (nir - red) / (nir + red)
}

ndvi_rast <- lapp(multi_rast[[c(4,3)]], fun = ndvi_fun)
```

#### Focal operations

-   Focal operations look at a central cell and its neighbors (adjacent cells)
    -   this 'neighborhood' typically 3-by-3 cells
        -   it is also called a 'kernel', 'filter', or 'moving window'
    -   Other names for focal operations include 'spatial filtering' or 'convolution'
-   The steps for focal operations in R are as such:
    -   define the shape of the window/neighborhood
        -   `w = matrix(1, nrow = 3, ncol = 3)` in the example below
    -   we specify the function to be applied to the neighborhood
        -   `fun = min` below
    -   we apply it to a raster using `focal()`, passing the raster, the window, and the function as arguments

```{r}
r_focal = focal(elev, w = matrix(1, nrow = 3, ncol = 3), fun = min)
```

-   Focal functions are used extensively in image processing
    -   low-pass filters or smoothing use `mean()` to remove extremes
    -   high-pass filters accentuate features
        -   e.g. Laplace and sobel filters
-   Terrain processing also relies on focal functions
    -   we use this to calculate slope, aspect, and flow directions
        -   `terrain()` is useful here

#### Zonal operations

-   Like focal operations, zonal operations apply an aggregation function
    -   unlike focal operations, zonal operations us a second raster with categorical values to act as 'zones'

```{r}
z = zonal(elev, grain, fun = "mean")
z

# by default the values are not returned as a raster; this can be done using as.raster in the zonal argument

zed = zonal(elev, grain, fun = "mean", as.raster = TRUE)
plot(zed)
```

#### Global operations and distances

-   global operations are a special case of zonal operations
    -   in the global case, the entire raster dataset acts as a single zone

#### Map algebra counterparts in vector processing

-   computing a distance raster with a maximum distance (global + logical focal) is equivalent to a vector buffer
-   reclassifying raster data is equivalent to dissolving vector data
-   raster masking is equivalent to vector clipping
-   raster spatial clipping is equivalent to vector intersect

### Merging rasters

-   often, rasters are split into more manageable 'scenes
    -   This mean that we often need to merge adjacent rasters to operate on them simultaneously as a single study area
-   we will download SRTM elevation data for Austria and Switzerland and merge them

```{r}
aut = geodata::elevation_30s(country = "AUT", path = tempdir())
ch = geodata::elevation_30s(country = "CHE", path = tempdir())
aut_ch = merge(aut, ch)
```

-   `terra::merge()` combines two images, adopting values from first raster in case of overlap
    -   This is not optimal in most cases - when combining spectral imagery from different dates, values will likely not be comensurate
        -   In this case, `mosaic` will likely be more useful
        -   `mosaic` allows you to define a function for overlap, such as mean

## Exercises

1.  It was established in section 4.2 that Canterbury was the region of New Zealand containing most of the 101 highest points in the country. How many of these high points does the Canterbury region contain? **Bonus**: plot the result using the `plot()` function to show all of New Zealand, `canterbury` highlighted in yellow, high points in Canterbury represented by red crosses (hint: `pch = 7`) and high points in other parts of New Zealand represented by blue circles. See help page `?points` for details.

    ```{r}
    cant <- nz %>% filter(Name == 'Canterbury')
    cant_count <- count(nz_height[cant,])
    not_cant_count <- count(nz_height[filter(nz, Name != 'Canterbury'),])

    nz_t <- st_union(nz, by_feature = FALSE)
    plot(nz_t)
    polys(cant, col = 'yellow')
    points(cant_count, 
         pch = 7,
         col = 'red'
         )
    points(not_cant_count,
         pch = 1,
         col = "blue",
         )
    print(cant_count$n)
    ```

2.  Which region has the second highest number of nz_height points, and how many does it have?

```{{r}}
tab <- st_join(nz_height, nz["Name"]) %>% 
  group_by(Name) %>%
  count(sort = TRUE)
tab[2,1:2]
```

3.  Generalizing the question to all regions: how many of New Zealand's 16 regions contain points which belong to the top 101 highest points in the country? Which regions? **Bonus**: create a a table listing these regions in order of number of points and their name.

    ```{r}
    tab %>% st_drop_geometry()
    ```

4.  Test your knowledge of spatial predicates by finding out and plotting how US states relate to each other and other spatial objects. The starting point of this exercise is to create an object representing Colorado state in the USA. Do this with the command `colorado = us_states[us_states$NAME == "Colorado",]` (base R) or with the `filter()` function (tidyverse) and plot the resulting object in the context of the US.

```{r}
colorado <- us_states %>% filter(NAME == "Colorado")
plot(us_states$geometry)
polys(colorado, col = "red")
```

```{r}
us_states[colorado,] %>% plot()
us_states[colorado, op = st_touches] %>% plot()

```

```{r warning=FALSE}
cali <- st_centroid(us_states[us_states$NAME == "California",])
dc <- st_centroid(us_states[us_states$NAME == "District of Columbia",])
dc_cali <- st_union(cali, dc) %>% st_cast(to = "LINESTRING")
plot(us_states$geometry)
lines(dc_cali, col = "red")
```

5.  Use `dem = rast(system.file("raster/dem.tif", package = "spDataLarge"))`, and reclassify the elevation into three classes:

    -   low: (\<300)
    -   medium
    -   high(\>500)

    Secondly, read the NDVI raster (`ndvi = rast(system.file("raster/ndvi.tif", package = "spDataLarge"))`) and compute the mean NDVI and the mean elevation for each altitudinal class.

6.  Apply a line detection filter to `rast(system.file("ex/logo.tif", package = "terra"))`. Plot the result. Hint: Read `?terra::focal()`.

7.  Calculate the Normalized Difference Water Index (NDWI: `(green - nir) / (green + nir)`) of a Landsat image. Use the Landsat image provided by the spDataLarge package (`system.file("raster/landsat.tif", package = "spDataLarge")`). Also, calculate a correlation between NDVI and NDWI for this area (hint: you can use the `layerCor()` function).

8.  A stack overflow post shows how to compute distances to the nearest coastline using `raster::distance()`. Try to do something similar but with `terra::distance()`: retrieve a digital elevation model of Spain, and compute a raster which represents distances to the coast across the country (hint: use `geodata:elevation_30s()`). Convert the resulting distances from meters to kilometers. Note: it may be wise to increase the cell size of the input raster to reduce compute time during this operation (`aggregate()`).

9.  Try to modify the approach used in the above exercise by weighting the distance raster with the elevation raster; every 100 altitudinal meters should increase the distance to the coast by 10 km. Next, compute and visualize the difference between the raster created using Euclidian distance (E7) and the raster weighted by elevation.
